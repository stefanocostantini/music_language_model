{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "language_model_first_try.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPkXxVmm54spiJJCuhk7joA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefanocostantini/music_language_model/blob/first-stab/language_model_first_try.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnKE3o2jwTvF",
        "colab_type": "text"
      },
      "source": [
        "## Music language model: first attempt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqaHCIDdoSeQ",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we try a simple language model, training it using our training dataset. The plan is to do the following:\n",
        "\n",
        "- Load the datasets from S3\n",
        "- Prepare the data, creating sequences of a pre-determined number of \"words\" (i.e. groups of notes, pre-padded when starting with the following piece) and trying to predict the next \"word\". When the sentence finishes, still try to predict the last \"word\" with fewers inputs\n",
        "- Set up an LSTM-based language model (simple to start with) and train it\n",
        "- Extract the encoding layer(s) and use them for classification, trying to predict the `composer` from the \"words\" in the piece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yme5chgvxSm",
        "colab_type": "text"
      },
      "source": [
        "### Setup and load datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cfacd-ipdyM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import boto3\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7McFlUHJQ6jG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters\n",
        "sequence_length = 15 # Length of encoded sequences to use in model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR71-qv-eCMv",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "827eab08-549e-41be-f7ce-975840aaaee4"
      },
      "source": [
        "# Load S3 access keys (need to manually add file)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3772930b-5e11-446b-b753-c6c9e1df257b\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-3772930b-5e11-446b-b753-c6c9e1df257b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving colab_accessKeys.csv to colab_accessKeys.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ggc2kIzJeeXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read access keys\n",
        "keys = pd.read_csv(\"colab_accessKeys.csv\")\n",
        "access = keys.iloc[0,0]\n",
        "secret = keys.iloc[0,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUDjYObCelMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up connection to S3\n",
        "s3 = boto3.client('s3', aws_access_key_id=access,aws_secret_access_key=secret)\n",
        "bucket = 'stefano-colab-data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM-IIdw3fW6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read train and test datasets\n",
        "key_prefix = 'datasets/'\n",
        "\n",
        "read_train = s3.get_object(Bucket=bucket, Key=key_prefix + \"training.csv\")\n",
        "train_raw = pd.read_csv(read_train['Body'], sep=',')\n",
        "\n",
        "read_test = s3.get_object(Bucket=bucket, Key=key_prefix + \"test.csv\")\n",
        "test_raw = pd.read_csv(read_test['Body'], sep = ',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIs_Scrcg5TJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "6415a6eb-c46b-4b27-f621-c7bcf691aee0"
      },
      "source": [
        "train_raw.head(3)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>composer</th>\n",
              "      <th>composition</th>\n",
              "      <th>movement</th>\n",
              "      <th>ensemble</th>\n",
              "      <th>source</th>\n",
              "      <th>transcriber</th>\n",
              "      <th>catalog_name</th>\n",
              "      <th>seconds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1727</td>\n",
              "      <td>41+53+65+69+81 41+53+65+69+81 41+53+65+69+81 4...</td>\n",
              "      <td>Schubert</td>\n",
              "      <td>Piano Quintet in A major</td>\n",
              "      <td>2. Andante</td>\n",
              "      <td>Piano Quintet</td>\n",
              "      <td>European Archive</td>\n",
              "      <td>http://tirolmusic.blogspot.com/</td>\n",
              "      <td>OP114</td>\n",
              "      <td>447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1728</td>\n",
              "      <td>69+81 69+81 73+85 73+85 74+86 74+86 45+45+45+4...</td>\n",
              "      <td>Schubert</td>\n",
              "      <td>Piano Quintet in A major</td>\n",
              "      <td>3. Scherzo: Presto</td>\n",
              "      <td>Piano Quintet</td>\n",
              "      <td>European Archive</td>\n",
              "      <td>http://tirolmusic.blogspot.com/</td>\n",
              "      <td>OP114</td>\n",
              "      <td>251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1729</td>\n",
              "      <td>69 69 69 69 69 69 69 69 38+57+62+66+74 38+57+6...</td>\n",
              "      <td>Schubert</td>\n",
              "      <td>Piano Quintet in A major</td>\n",
              "      <td>4. Andantino - Allegretto</td>\n",
              "      <td>Piano Quintet</td>\n",
              "      <td>European Archive</td>\n",
              "      <td>http://tirolmusic.blogspot.com/</td>\n",
              "      <td>OP114</td>\n",
              "      <td>444</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id  ... seconds\n",
              "0  1727  ...     447\n",
              "1  1728  ...     251\n",
              "2  1729  ...     444\n",
              "\n",
              "[3 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLqEUoe5v2FW",
        "colab_type": "text"
      },
      "source": [
        "### Prepare data for model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qvl4Sf9lvzOs",
        "colab_type": "text"
      },
      "source": [
        "#### _Tokenize words and convert strings into integer sequences_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEDOhn4NysMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First we create a variable, called 'texts' (train and test) which will contain all the strings\n",
        "# in the training dataset. We will also collect the labels (to be used later)\n",
        "texts_train = []\n",
        "labels_train = []\n",
        "for index, row in train_raw.iterrows():\n",
        "    texts_train.append(row.text)\n",
        "    labels_train.append(row.composer)\n",
        "\n",
        "texts_test = []\n",
        "labels_test = []\n",
        "for index, row in test_raw.iterrows():\n",
        "    texts_test.append(row.text)\n",
        "    labels_test.append(row.composer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoBel60y7Is8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We now need to convert the sequences of \"words\" into sequence of integers. \n",
        "# We do this by using the Keras tokenizer. The default includes the '+' sign as\n",
        "# one of the filters to be removed. This is not appropriate here as \"words\" are\n",
        "# defined as 'note1+note2+...'. So we remove the '+' sign from the filter when\n",
        "# instantiating this class.\n",
        "tk = Tokenizer(filters='!\"#$%&()*,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG6XFq3p-fxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To make sure we have a consistent dictionary, we apply the tokenizer on to the\n",
        "# entire corpus\n",
        "texts_all = texts_train + texts_test\n",
        "tk.fit_on_texts(texts_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqM6A1C2-f2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now let's convert the texts into sequences of integers\n",
        "sequences_train = tk.texts_to_sequences(texts_train)\n",
        "sequences_test = tk.texts_to_sequences(texts_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFwhRtrBB7_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We then determine the size of the vocabulary, which will be needed to specify\n",
        "# the size of the word embedding layer(s) of the model and for encoding output\n",
        "# words using one-hot encoding\n",
        "vocab_size = len(tk.word_index) + 1 # adding 1 as the vocabulary index will have len + 1 positions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueZK0afGv_7R",
        "colab_type": "text"
      },
      "source": [
        "#### _Create sequences and target \"words\" for language model_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4IwAtiywRqX",
        "colab_type": "text"
      },
      "source": [
        "We want to create sequences of a pre-determined number of words which the model will use to learn what the word after the sequence should be. The length of the sequence should be  a variable. \n",
        "\n",
        "When extracting a sequence, we will also extract an extra word at the end, which we will later split to make that the target word for each sequence.\n",
        "\n",
        "Sequences for the model are defined only within each piece (as it would not make sense to predict the first notes of a new piece using the last notes of another) (_We may test this in future_)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4S9wdBsB-iq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model_sequences(orig_sequences, max_sequence_length):\n",
        "  \"\"\"\n",
        "  Given sequences of text, for each it builds model sequences of words of the\n",
        "  desired length (+1 for the target word). No model sequence is build using words\n",
        "  from more than one of the original sequences.\n",
        "  \"\"\"\n",
        "  for sequence in orig_sequences:\n",
        "    model_sequences = list()\n",
        "    for i in range(max_sequence_length, len(sequence)):\n",
        "      seq = sequence[i-max_sequence_length:i+1]\n",
        "      model_sequences.append(seq)\n",
        "  return model_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVCacZr6MZSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Applying the function above we construct sequences of a chosen length. These \n",
        "train_all = build_model_sequences(sequences_train, sequence_length)\n",
        "test_all = build_model_sequences(sequences_test, sequence_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yPnz3zBRC0Y",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vv9hTbIM1J9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The last step involves separating the actual sequence from the target word\n",
        "# in each case and one-hot encoding the target word\n",
        "train_all = np.array(train_all)\n",
        "test_all = np.array(test_all)\n",
        "X_train, y_train = train_all[:,:-1], train_all[:,-1]\n",
        "X_test, y_test = test_all[:,:-1], test_all[:,-1]\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=vocab_size)\n",
        "y_test = to_categorical(y_test, num_classes=vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJmDbDPXv8q2",
        "colab_type": "text"
      },
      "source": [
        "### Set up language model and train, evaluating performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVXVdTuHwBjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkpDrqGdwCIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lOi--r9wChu",
        "colab_type": "text"
      },
      "source": [
        "### Extract encoding from language model and train a classifier to identify the composer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_59vM0pRwJ4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K6Pi3SDwKax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAPjuJ3awKl6",
        "colab_type": "text"
      },
      "source": [
        "### Train classifier directly on the composer labels and the same featurised data to compare performance with the previous approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BI4B6B6HwRul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}